# coding: utf-8

"""
    Dalet Media Cortex API

    # Scope Dalet Mediator API allows you to submit long running media jobs managed by Dalet services.  Long running media jobs include: - **Media processing** such as transcoding or automatic QC. - **Automatic metadata extraction** such as automatic speech transcription or face detection.  The Dalet Mediator API is a REST API with typed schema for the payload. # Architecture Job processing is performed on the cloud via dynamic combination of microservices. Dalet Mediator adopts the [EBU MCMA] architecture.  The key objectives of this architecture are to support: - Job management and monitoring - Long running transactions - Event based communication pattern - Service registration and discovery - Horizontal scalability in an elastic manner  The architecture is implemented using the serverless approach - relying on  independent microservices accessible through well documented REST endpoints and sharing a common object model. ## Roles The following services are involved in the processing of media jobs exposed through the Dalet Media Mediator API: - **Mediator**: this is the main entry point to the architecture; this API endpoint supports: 1. Checking authentication using an API key and a token mechanism 2. Verifying quota restrictions before accepting a submitted job 3. Keeping track of usage so that job processing can be tracked and billed 4. Keeping track of jobs metadata as a job repository - **Job Processor**: once a job request is accepted by the mediator, it is assigned to a Job Processor. The Job Processor dispatches the job to an appropriate Job Worker (depending on the job profile and other criteria such as load on the system and cost of operation).  It then keeps track of the progress of the job and its status until completion and possible failures and timeout.  It reports progress to the Mediator through notifications. - **Job Worker**: The Job Worker performs the actual work on the media object, for example, AI metadata extraction (AME) or essence transcoding.  It reports progress to the Job Processor through notifications. - **Service Registry**: The Service Registry keeps track of all active services in the architecture. It is queried by the Mediator and by Processors to discover candidate services to perform jobs.  It is updated whenever a new service is launched or stopped.  The Service Registry also stores the list of all job profiles supported by one of the Job Workers deployed in the architecture. The Dalet Mediator API abstracts away from the complexity of this orchestration and provides a simple endpoint to submit long running jobs and monitor the progress of their execution.  It serves as a facade for the additional technical services for authentication, usage monitoring and service registry.  [EBU MCMA]: /https://tech.ebu.ch/groups/mcma 'EBU MCMA' ## Job Lifecycle ![Job Lifecyle Diagram](./job_lifecycle.svg 'Job Lifecycle Diagram')  ## Authentication To use the Dalet Mediator API - you must obtain an APIKey from Dalet.  This key comes in the form of two parameters: * client ID * secret  Given these two parameters, a client program must first obtain an access token (GET /auth/access-token) and then associate this token to every subsequent calls.  When the token expires, the API will return a 401 error code.  In this case, the client must request a new token and resubmit the request. 

    The version of the OpenAPI document: 2.1.0
    Contact: cortexsupport@dalet.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import ConfigDict, Field, StrictFloat, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from cortex_client.models.job_input import JobInput
from cortex_client.models.locator import Locator
from cortex_client.models.speech_to_text_output import SpeechToTextOutput
from cortex_client.models.speech_to_text_vocabulary import SpeechToTextVocabulary
from typing import Optional, Set
from typing_extensions import Self

class SpeechToTextInput(JobInput):
    """
    Describe the input and expected output of the job to be executed by Media Cortex. Files are specified using FIMS Essence Locators. Specifies which media file is to be indexed by the Media Cortex service, its language, and a user defined name associated to the job.  This job produces captions for the media in EBU-TT, JSON and textual formats
    """ # noqa: E501
    input_file: Locator = Field(alias="inputFile")
    output_location: SpeechToTextOutput = Field(alias="outputLocation")
    title: Optional[StrictStr] = Field(default=None, description="Readable name associated to the job when submitted to Media Cortex service.")
    language: Optional[StrictStr] = Field(default=None, description="Language of the media file to be indexed according to ISO 639-1 Language Name. See https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.  Used in Job/inputFile")
    start_of_material: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Indicate the timecode of the first frame in the media in seconds.  Generated subtitles start from this time.", alias="startOfMaterial")
    vocabulary: Optional[List[SpeechToTextVocabulary]] = None
    caption_format_standard: Optional[StrictStr] = Field(default=None, description="Which standard will be used for caption formatting. EBU-TT, CEA-608 and CEA-708 are supported standards.", alias="captionFormatStandard")
    __properties: ClassVar[List[str]] = ["jobInputType", "inputFile", "outputLocation", "title", "language", "startOfMaterial", "vocabulary", "captionFormatStandard"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of SpeechToTextInput from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of input_file
        if self.input_file:
            _dict['inputFile'] = self.input_file.to_dict()
        # override the default output from pydantic by calling `to_dict()` of output_location
        if self.output_location:
            _dict['outputLocation'] = self.output_location.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in vocabulary (list)
        _items = []
        if self.vocabulary:
            for _item_vocabulary in self.vocabulary:
                if _item_vocabulary:
                    _items.append(_item_vocabulary.to_dict())
            _dict['vocabulary'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of SpeechToTextInput from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "jobInputType": obj.get("jobInputType"),
            "inputFile": Locator.from_dict(obj["inputFile"]) if obj.get("inputFile") is not None else None,
            "outputLocation": SpeechToTextOutput.from_dict(obj["outputLocation"]) if obj.get("outputLocation") is not None else None,
            "title": obj.get("title"),
            "language": obj.get("language"),
            "startOfMaterial": obj.get("startOfMaterial"),
            "vocabulary": [SpeechToTextVocabulary.from_dict(_item) for _item in obj["vocabulary"]] if obj.get("vocabulary") is not None else None,
            "captionFormatStandard": obj.get("captionFormatStandard")
        })
        return _obj


